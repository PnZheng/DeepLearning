{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/PnZheng/DeepLearning/blob/main/RNN/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "我们将使用PyTorch来训练RNN模型，来简单的实现文本分类任务(情感分析)。\n",
    "\n",
    "- Input: 模型中将一段文本作为输入\n",
    "- Output: 并输出'1'表示积极情绪，'0'表示消息情绪。  \n",
    "- 在这里我们使用单层单向的RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些必要的包的引入\n",
    "- punctuation 和 Counter 用于文本处理、matplotlib用于展示数据\n",
    "- numpy 用于数组操作、tqdm用于可视化进度条\n",
    "- torchsummary用于展示网络的结构层次及相关数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x203f4c53a30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 设置生成随机数的种子，保证实验的可重复性\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集的操作:\n",
    "\n",
    "- 你首先需要在这里下载数据集: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "- 然后，需要将下载好的包解压到现在的工作目录下\n",
    "- 解压完成后，会出一线一个名为\"ac1Imdb\"的文件出现在工作目录下\n",
    "- 这个数据集是由一些电影评论作为文本输入的，并且有着相对应的情绪标签\n",
    "- 此外，也可以使用下面的命令行进行下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar zxvf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取文本文件中的情感标签和评论数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:04<00:00, 2970.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:04<00:00, 2595.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews : 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "review_list = []\n",
    "label_list = []\n",
    "for label in ['pos', 'neg']:\n",
    "    for fname in tqdm(os.listdir(f'./aclImdb/train/{label}/')):\n",
    "        if 'txt' not in fname:\n",
    "            continue\n",
    "        with open(os.path.join(f'./aclImdb/train/{label}/', fname), encoding=\"utf8\") as f:\n",
    "            review_list += [f.read()]\n",
    "            label_list += [label]\n",
    "print ('Number of reviews :', len(review_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件的预处理\n",
    "- 将数据全转为小写\n",
    "- 排除掉特殊字符\n",
    "- 并将所有影评连接在一起后分组\n",
    "- 记录词汇的频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25000/25000 [00:02<00:00, 11887.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 334691), ('and', 162228), ('a', 161940), ('of', 145326), ('to', 135042), ('is', 106855), ('in', 93028), ('it', 77099), ('i', 75719), ('this', 75190)]\n"
     ]
    }
   ],
   "source": [
    "review_list = [review.lower() for review in review_list]\n",
    "review_list = [''.join([letter for letter in review if letter not in punctuation]) for review in tqdm(review_list)]\n",
    "\n",
    "reviews_blob = ' '.join(review_list)\n",
    "\n",
    "review_words = reviews_blob.split()\n",
    "# get the word counts\n",
    "count_words = Counter(review_words)\n",
    "\n",
    "total_review_words = len(review_words)\n",
    "sorted_review_words = count_words.most_common(total_review_words)\n",
    "\n",
    "print(sorted_review_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将单词转化为数字序列，因为机器学习中模型仅对数字能理解解释，而并非单词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('and', 2), ('a', 3), ('of', 4), ('to', 5), ('is', 6), ('in', 7), ('it', 8), ('i', 9), ('this', 10)]\n"
     ]
    }
   ],
   "source": [
    "vocab_to_token = {word:idx+1 for idx, (word, count) in enumerate(sorted_review_words)}\n",
    "print(list(vocab_to_token.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到了单词对应的字典后，将原来的文本进行单词替换为数字。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt\n",
      "\n",
      "[22572, 321, 6, 3, 1077, 219, 8, 2082, 30, 1, 166, 61, 14, 46, 80, 5581, 42, 399, 118, 135, 14, 4883, 55, 4980, 147, 7, 1, 4941, 6023, 479, 69, 5, 255, 11, 22572, 17217, 1970, 6, 72, 2356, 5, 638, 70, 6, 4883, 1, 26241, 5, 2031, 10833, 1, 5884, 1421, 36, 68, 67, 204, 140, 64, 1215, 4883, 21183, 1, 43770, 4, 1, 218, 902, 31, 2922, 69, 4, 1, 4706, 9, 671, 2, 64, 1421, 50, 9, 207, 1, 382, 7, 59, 3, 1473, 3614, 774, 5, 3561, 186, 1, 399, 9, 1191, 14623, 30, 321, 3, 349, 362, 2960, 141, 131, 5, 9055, 28, 4, 122, 4883, 1473, 2410, 5, 22572, 321, 9, 515, 11, 105, 1462, 4, 55, 580, 102, 11, 22572, 321, 6, 233, 8881, 48, 3, 2285, 11, 8, 206]\n"
     ]
    }
   ],
   "source": [
    "reviews_tokenized = []\n",
    "for review in review_list:\n",
    "    word_to_token = [vocab_to_token[word] for word in review.split()]\n",
    "    reviews_tokenized.append(word_to_token)\n",
    "print(review_list[0])\n",
    "print()\n",
    "print (reviews_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode sentiments as 0 or 1\n",
    "encoded_label_list = [1 if label =='pos' else 0 for label in label_list]\n",
    "\n",
    "reviews_len = [len(review) for review in reviews_tokenized]\n",
    "\n",
    "reviews_tokenized = [reviews_tokenized[i] for i, l in enumerate(reviews_len) if l>0 ]\n",
    "encoded_label_list = np.array([encoded_label_list[i] for i, l in enumerate(reviews_len) if l> 0 ], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不同的review有不同 的长度，我们需要将他固定序列长度以定义简单的RNN模型， 这里设置Length = 512。\n",
    "\n",
    "### 我们对过短的进行填补，对过长的进行裁剪。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.7708e+04, 5.3030e+03, 1.3860e+03, 5.3200e+02, 5.6000e+01,\n",
       "        8.0000e+00, 4.0000e+00, 2.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([  10. ,  255.9,  501.8,  747.7,  993.6, 1239.5, 1485.4, 1731.3,\n",
       "        1977.2, 2223.1, 2469. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pad_sequence(reviews_tokenized, sequence_length):\n",
    "    ''' returns the tokenized review sequences padded with 0's or truncated to the sequence_length.\n",
    "    '''\n",
    "    padded_reviews = np.zeros((len(reviews_tokenized), sequence_length), dtype = int)\n",
    "    \n",
    "    for idx, review in enumerate(reviews_tokenized):\n",
    "        review_len = len(review)\n",
    "        if review_len <= sequence_length:\n",
    "            zeroes = list(np.zeros(sequence_length-review_len))\n",
    "            new_sequence = zeroes+review\n",
    "        elif review_len > sequence_length:\n",
    "            new_sequence = review[0:sequence_length]\n",
    "        \n",
    "        padded_reviews[idx,:] = np.array(new_sequence)\n",
    "    \n",
    "    return padded_reviews\n",
    "\n",
    "sequence_length = 512\n",
    "padded_reviews = pad_sequence(reviews_tokenized=reviews_tokenized, sequence_length=sequence_length)\n",
    "# 展示直方图\n",
    "plt.rcParams['axes.unicode_minus'] = False # 解决中文显示问题\n",
    "plt.title(f'词频直方图')\n",
    "plt.hist(reviews_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在这里，我们以75:25的比例将数据集分为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = 0.75\n",
    "train_X = padded_reviews[:int(train_val_split*len(padded_reviews))]\n",
    "train_y = encoded_label_list[:int(train_val_split*len(padded_reviews))]\n",
    "validation_X = padded_reviews[int(train_val_split*len(padded_reviews)):]\n",
    "validation_y = encoded_label_list[int(train_val_split*len(padded_reviews)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If while training, you get a runtime error that says: \"RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long\".\n",
    "## simply uncomment run the following lines of code additionally\n",
    "# train_X = train_X.astype('int64')\n",
    "# train_y = train_y.astype('int64')\n",
    "# validation_X = validation_X.astype('int64')\n",
    "# validation_y = validation_y.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成数据集及dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(train_X).to(device), torch.from_numpy(train_y).to(device))\n",
    "validation_dataset = TensorDataset(torch.from_numpy(validation_X).to(device), torch.from_numpy(validation_y).to(device))\n",
    "\n",
    "batch_size = 32\n",
    "# torch dataloaders (shuffle data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取其中一个批次的数据显示看看\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input size:  torch.Size([32, 512])\n",
      "Example Input:\n",
      " tensor([[    0,     0,     0,  ...,  4685,     1,  1464],\n",
      "        [    0,     0,     0,  ...,  2342,     2,  2715],\n",
      "        [    0,     0,     0,  ...,     7,    10,    19],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,  1515,    29, 91311],\n",
      "        [    0,     0,     0,  ...,    98,    80,    94],\n",
      "        [    3,   335,    41,  ...,  1268,     2,    55]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "\n",
      "Example Output size:  torch.Size([32])\n",
      "Example Output:\n",
      " tensor([0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_data_iter = iter(train_dataloader)\n",
    "X_example, y_example = train_data_iter.next()\n",
    "print('Example Input size: ', X_example.size()) # batch_size, seq_length\n",
    "print('Example Input:\\n', X_example)\n",
    "print()\n",
    "print('Example Output size: ', y_example.size()) # batch_size\n",
    "print('Example Output:\\n', y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 模型的定义\n",
    "- sequence shape = (sequence_length, batch_size)\n",
    "- embedding shape = [sequence_length, batch_size, embedding_dimension]\n",
    "- output shape = [sequence_length, batch_size, hidden_dimension]\n",
    "- hidden_state shape = [1, batch_size, hidden_dimension]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dimension, embedding_dimension, hidden_dimension, output_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(input_dimension, embedding_dimension)  \n",
    "        self.rnn_layer = nn.RNN(embedding_dimension, hidden_dimension, num_layers=1)\n",
    "        self.fc_layer = nn.Linear(hidden_dimension, output_dimension)\n",
    "        \n",
    "    def forward(self, sequence):\n",
    "        embedding = self.embedding_layer(sequence)  \n",
    "        output, hidden_state = self.rnn_layer(embedding)\n",
    "        final_output = self.fc_layer(hidden_state[-1,:,:].squeeze(0))      \n",
    "        return final_output\n",
    "    \n",
    "input_dimension = len(vocab_to_token)+1 # +1 to account for padding\n",
    "embedding_dimension = 100\n",
    "hidden_dimension = 32\n",
    "output_dimension = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化函数 和 损失函数的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN(input_dimension, embedding_dimension, hidden_dimension, output_dimension)\n",
    "optim = torch.optim.Adam(rnn_model.parameters())\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "rnn_model = rnn_model.to(device)\n",
    "loss_func = loss_func.to(device)\n",
    "\n",
    "#summary(rnn_model,np.asscalar(np.int16(1,3,28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义准确度指标来衡量我们的训练模型在验证集上的性能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns 0-1 accuracy for the given set of predictions and ground truth\n",
    "    \"\"\"\n",
    "    # round predictions to either 0 or 1\n",
    "    rounded_predictions = torch.round(torch.sigmoid(predictions))\n",
    "    success = (rounded_predictions == ground_truth).float() #convert into float for division \n",
    "    accuracy = success.sum() / len(success)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optim, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.train()\n",
    "    \n",
    "    for sequence, sentiment in dataloader:\n",
    "        # 清除梯度，以防上一次使用的参与这次训练之中\n",
    "        optim.zero_grad()     \n",
    "        preds = model(sequence.T).squeeze()\n",
    "        \n",
    "        loss_curr = loss_func(preds, sentiment)\n",
    "        accuracy_curr = accuracy_metric(preds, sentiment)\n",
    "        \n",
    "        loss_curr.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss += loss_curr.item()\n",
    "        accuracy += accuracy_curr.item()\n",
    "        \n",
    "    return loss/len(dataloader), accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证过程函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequence, sentiment in dataloader:\n",
    "            \n",
    "            preds = model(sequence.T).squeeze()\n",
    "            \n",
    "            loss_curr = loss_func(preds, sentiment)   \n",
    "            accuracy_curr = accuracy_metric(preds, sentiment)\n",
    "\n",
    "            loss += loss_curr.item()\n",
    "            accuracy += accuracy_curr.item()\n",
    "        \n",
    "    return loss/len(dataloader), accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练模型并保存最好的模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 1 | time elapsed: 45.824301958084106s\n",
      "training loss: 0.616 | training accuracy: 67.24%\n",
      "validation loss: 1.109 |  validation accuracy: 20.66%\n",
      "\n",
      "epoch number: 2 | time elapsed: 42.594305753707886s\n",
      "training loss: 0.529 | training accuracy: 74.04%\n",
      "validation loss: 0.958 |  validation accuracy: 39.49%\n",
      "\n",
      "epoch number: 3 | time elapsed: 42.712043046951294s\n",
      "training loss: 0.437 | training accuracy: 80.68%\n",
      "validation loss: 0.844 |  validation accuracy: 56.73%\n",
      "\n",
      "epoch number: 4 | time elapsed: 42.7351713180542s\n",
      "training loss: 0.370 | training accuracy: 84.52%\n",
      "validation loss: 1.012 |  validation accuracy: 52.46%\n",
      "\n",
      "epoch number: 5 | time elapsed: 42.71858263015747s\n",
      "training loss: 0.306 | training accuracy: 87.91%\n",
      "validation loss: 0.731 |  validation accuracy: 68.48%\n",
      "\n",
      "epoch number: 6 | time elapsed: 42.67591142654419s\n",
      "training loss: 0.244 | training accuracy: 90.79%\n",
      "validation loss: 0.957 |  validation accuracy: 62.59%\n",
      "\n",
      "epoch number: 7 | time elapsed: 42.66887354850769s\n",
      "training loss: 0.237 | training accuracy: 90.94%\n",
      "validation loss: 1.194 |  validation accuracy: 57.97%\n",
      "\n",
      "epoch number: 8 | time elapsed: 42.66598629951477s\n",
      "training loss: 0.177 | training accuracy: 93.75%\n",
      "validation loss: 1.257 |  validation accuracy: 56.64%\n",
      "\n",
      "epoch number: 9 | time elapsed: 42.6548228263855s\n",
      "training loss: 0.145 | training accuracy: 94.90%\n",
      "validation loss: 1.282 |  validation accuracy: 56.92%\n",
      "\n",
      "epoch number: 10 | time elapsed: 42.89893579483032s\n",
      "training loss: 0.119 | training accuracy: 96.09%\n",
      "validation loss: 1.239 |  validation accuracy: 61.85%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    time_start = time.time()\n",
    "    \n",
    "    training_loss, train_accuracy = train(rnn_model, train_dataloader, optim, loss_func)\n",
    "    validation_loss, validation_accuracy = validate(rnn_model, validation_dataloader, loss_func)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_delta = time_end - time_start  \n",
    "    # 保存模型参数\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(rnn_model.state_dict(), 'rnn_model.pt')\n",
    "    \n",
    "    print(f'epoch number: {ep+1} | time elapsed: {time_delta}s')\n",
    "    print(f'training loss: {training_loss:.3f} | training accuracy: {train_accuracy*100:.2f}%')\n",
    "    print(f'validation loss: {validation_loss:.3f} |  validation accuracy: {validation_accuracy*100:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们也可以定义一个辅助函数来对训练后的模型进行实时推理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_inference(model, sentence):\n",
    "    model.eval()\n",
    "    \n",
    "    # text transformations\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([c for c in sentence if c not in punctuation])\n",
    "    tokenized = [vocab_to_token.get(token, 0) for token in sentence.split()]\n",
    "    tokenized = np.pad(tokenized, (512-len(tokenized), 0), 'constant')\n",
    "    \n",
    "    # model inference\n",
    "    model_input = torch.LongTensor(tokenized).to(device)\n",
    "    model_input = model_input.unsqueeze(1)\n",
    "    pred = torch.sigmoid(model(model_input))\n",
    "    \n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对一些简单的句子进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002798324276227504\n",
      "0.010395572520792484\n",
      "0.04620944708585739\n",
      "0.6425880789756775\n",
      "0.22223013639450073\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_inference(rnn_model, \"This film is horrible\"))\n",
    "print(sentiment_inference(rnn_model, \"Director tried too hard but this film is bad\"))\n",
    "print(sentiment_inference(rnn_model, \"Decent movie, although could be shorter\"))\n",
    "print(sentiment_inference(rnn_model, \"This film will be houseful for weeks\"))\n",
    "print(sentiment_inference(rnn_model, \"I loved the movie, every part of it\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "- 在训练过程中，能够看到训练的准确度很高，但是验证的普遍较低，这是什么原因造成的？\n",
    "- 如果我们通过修改Loss function后，结果会发送什么变化？\n",
    "- 如果调整批大小后，本节的代码结果会发生变化？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
