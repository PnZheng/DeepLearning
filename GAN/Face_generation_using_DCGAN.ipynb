{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PacktPublishing/Hands-On-Computer-Vision-with-PyTorch/blob/master/Chapter12/Face_generation_using_DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "常用方法中利用wget进行数据集下载，或者自己手动下载并解压。\n",
    "已下载好的数据集也放在目录中，可自行使用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI3WwhKvfHgm"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/rbajpdlh7efkdo1/male_female_face_images.zip\n",
    "!unzip -q male_female_face_images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些常用的包引入，这里可能需要对torch_snippets利用pip进行下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WeJpNSnLfLr5",
    "outputId": "f2b38c50-83ea-4a95-e2fa-13a32ebbca91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-06 00:09:56.008 | WARNING  | torch_snippets:<module>:13 - sklearn is not found. Skipping relevant imports from submodule `sklegos`\n",
      "Exception: No module named 'sklego'\n"
     ]
    }
   ],
   "source": [
    "from torch_snippets import *\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "import cv2, numpy as np, pandas as pd\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下载的数据集中，为了只保留面部并丢弃图像中的其他细节，我们因此需要裁剪图像。\n",
    "\n",
    "首先，我们将下载级联过滤器（ OpenCV 进行图像分析中有关 OpenCV 中的级联过滤器的更多信息），这将有助于识别图像中的人脸："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MxnHjD0bfNnP"
   },
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里利用级联过滤器，来将数据集中人脸图像进行裁剪，整理到cropped_faces目录中去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4toje9ZyfPJw",
    "outputId": "1a773729-e424-433e-eb7c-67d78c9524ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file cropped_faces already exists.\n",
      "2021-09-05 23:23:39.023 | INFO     | torch_snippets.paths:inner:24 - 14688 files found at ./face_images/females/*.jpg\n",
      "2021-09-05 23:23:39.225 | INFO     | torch_snippets.paths:inner:24 - 13948 files found at ./face_images/males/*.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-52d0ced14fec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mimg2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cropped_faces/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.jpg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_RGB2BGR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!mkdir cropped_faces\n",
    "images = Glob('./face_images/females/*.jpg')+Glob('./face_images/males/*.jpg')\n",
    "for i in range(len(images)):\n",
    "    img = read(images[i],1)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        img2 = img[y:(y+h),x:(x+w),:]\n",
    "    cv2.imwrite('cropped_faces/'+str(i)+'.jpg',cv2.cvtColor(img2, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义transform来对展示每一张图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XqeA3FRrfRaW"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "                               transforms.Resize(64),\n",
    "                               transforms.CenterCrop(64),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个Faces类，来对数据集进行管理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0iSU5PzTfhlu"
   },
   "outputs": [],
   "source": [
    "class Faces(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        super().__init__()\n",
    "        self.folder = folder\n",
    "        self.images = sorted(Glob(folder))\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, ix):\n",
    "        image_path = self.images[ix]\n",
    "        image = Image.open(image_path)\n",
    "        image = transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集的URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gX1o-X16fil0",
    "outputId": "e6074faa-9940-4c03-ecd2-de9ad29a10af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-06 00:10:18.441 | INFO     | torch_snippets.paths:inner:24 - 28636 files found at cropped_faces/\n"
     ]
    }
   ],
   "source": [
    "ds = Faces(folder='cropped_faces/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个批大小为64，进行shuffle的数据加载器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M27gcVo8f5ei"
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds, batch_size=64, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重初始化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jmt_Saeef7JJ"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判别器定义\n",
    "其中利用LeakyReLU函数当激活函数。并且利用BatchNorma2d进行归一化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Pst13pIvf8vy"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3,64,4,2,1,bias=False),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64,64*2,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*2),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64*2,64*4,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*4),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64*4,64*8,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*8),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64*8,1,4,1,0,bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "    def forward(self, input): return self.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对判别器进行实例化，并安装torch_summary包来将判别器的结构进行显示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FV_-iDJGf-53",
    "outputId": "bac84ebb-e87b-405c-a92c-03a385eac39a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: torch_summary in e:\\anaconda3\\lib\\site-packages (1.4.5)\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 1, 1, 1]             --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 32, 32]          3,072\n",
      "|    └─LeakyReLU: 2-2                    [-1, 64, 32, 32]          --\n",
      "|    └─Conv2d: 2-3                       [-1, 128, 16, 16]         131,072\n",
      "|    └─BatchNorm2d: 2-4                  [-1, 128, 16, 16]         256\n",
      "|    └─LeakyReLU: 2-5                    [-1, 128, 16, 16]         --\n",
      "|    └─Conv2d: 2-6                       [-1, 256, 8, 8]           524,288\n",
      "|    └─BatchNorm2d: 2-7                  [-1, 256, 8, 8]           512\n",
      "|    └─LeakyReLU: 2-8                    [-1, 256, 8, 8]           --\n",
      "|    └─Conv2d: 2-9                       [-1, 512, 4, 4]           2,097,152\n",
      "|    └─BatchNorm2d: 2-10                 [-1, 512, 4, 4]           1,024\n",
      "|    └─LeakyReLU: 2-11                   [-1, 512, 4, 4]           --\n",
      "|    └─Conv2d: 2-12                      [-1, 1, 1, 1]             8,192\n",
      "|    └─Sigmoid: 2-13                     [-1, 1, 1, 1]             --\n",
      "==========================================================================================\n",
      "Total params: 2,765,568\n",
      "Trainable params: 2,765,568\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 106.58\n",
      "==========================================================================================\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 1.38\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 11.97\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_summary\n",
    "from torchsummary import summary\n",
    "discriminator = Discriminator().to(device)\n",
    "summary(discriminator,torch.zeros(1,3,64,64));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成器的定义\n",
    "这里利用的是ReLU函数当激活函数。并且利用ConvTranspose2d对进行逆卷计划操作。\n",
    "其中输入的大小为：批大小 * 100 * 1 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BYZZuAW-gAvU"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100,64*8,4,1,0,bias=False,),\n",
    "            nn.BatchNorm2d(64*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64*8,64*4,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( 64*4,64*2,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( 64*2,64,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( 64,3,4,2,1,bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "    def forward(self,input): return self.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化并显示其结构:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPLs8OQLgD0-",
    "outputId": "c092fa56-7abb-4994-9d4e-1ec224af2539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 3, 64, 64]           --\n",
      "|    └─ConvTranspose2d: 2-1              [-1, 512, 4, 4]           819,200\n",
      "|    └─BatchNorm2d: 2-2                  [-1, 512, 4, 4]           1,024\n",
      "|    └─ReLU: 2-3                         [-1, 512, 4, 4]           --\n",
      "|    └─ConvTranspose2d: 2-4              [-1, 256, 8, 8]           2,097,152\n",
      "|    └─BatchNorm2d: 2-5                  [-1, 256, 8, 8]           512\n",
      "|    └─ReLU: 2-6                         [-1, 256, 8, 8]           --\n",
      "|    └─ConvTranspose2d: 2-7              [-1, 128, 16, 16]         524,288\n",
      "|    └─BatchNorm2d: 2-8                  [-1, 128, 16, 16]         256\n",
      "|    └─ReLU: 2-9                         [-1, 128, 16, 16]         --\n",
      "|    └─ConvTranspose2d: 2-10             [-1, 64, 32, 32]          131,072\n",
      "|    └─BatchNorm2d: 2-11                 [-1, 64, 32, 32]          128\n",
      "|    └─ReLU: 2-12                        [-1, 64, 32, 32]          --\n",
      "|    └─ConvTranspose2d: 2-13             [-1, 3, 64, 64]           3,072\n",
      "|    └─Tanh: 2-14                        [-1, 3, 64, 64]           --\n",
      "==========================================================================================\n",
      "Total params: 3,576,704\n",
      "Trainable params: 3,576,704\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 431.92\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.97\n",
      "Params size (MB): 13.64\n",
      "Estimated Total Size (MB): 15.61\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 3, 64, 64]           --\n",
       "|    └─ConvTranspose2d: 2-1              [-1, 512, 4, 4]           819,200\n",
       "|    └─BatchNorm2d: 2-2                  [-1, 512, 4, 4]           1,024\n",
       "|    └─ReLU: 2-3                         [-1, 512, 4, 4]           --\n",
       "|    └─ConvTranspose2d: 2-4              [-1, 256, 8, 8]           2,097,152\n",
       "|    └─BatchNorm2d: 2-5                  [-1, 256, 8, 8]           512\n",
       "|    └─ReLU: 2-6                         [-1, 256, 8, 8]           --\n",
       "|    └─ConvTranspose2d: 2-7              [-1, 128, 16, 16]         524,288\n",
       "|    └─BatchNorm2d: 2-8                  [-1, 128, 16, 16]         256\n",
       "|    └─ReLU: 2-9                         [-1, 128, 16, 16]         --\n",
       "|    └─ConvTranspose2d: 2-10             [-1, 64, 32, 32]          131,072\n",
       "|    └─BatchNorm2d: 2-11                 [-1, 64, 32, 32]          128\n",
       "|    └─ReLU: 2-12                        [-1, 64, 32, 32]          --\n",
       "|    └─ConvTranspose2d: 2-13             [-1, 3, 64, 64]           3,072\n",
       "|    └─Tanh: 2-14                        [-1, 3, 64, 64]           --\n",
       "==========================================================================================\n",
       "Total params: 3,576,704\n",
       "Trainable params: 3,576,704\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 431.92\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1.97\n",
       "Params size (MB): 13.64\n",
       "Estimated Total Size (MB): 15.61\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator().to(device)\n",
    "summary(generator,torch.zeros(1,100,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练过程中生成器和判别器的处理函数\n",
    "\n",
    "这里利用squeeze操作将输出模型预测的值转换为batchsize * 1 * 1 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DMmN1_K8gFr7"
   },
   "outputs": [],
   "source": [
    "def discriminator_train_step(real_data, fake_data):\n",
    "    d_optimizer.zero_grad()\n",
    "    prediction_real = discriminator(real_data)\n",
    "    error_real = loss(prediction_real.squeeze(), torch.ones(len(real_data)).to(device))\n",
    "    error_real.backward()\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    error_fake = loss(prediction_fake.squeeze(), torch.zeros(len(fake_data)).to(device))\n",
    "    error_fake.backward()\n",
    "    d_optimizer.step()\n",
    "    return error_real + error_fake\n",
    "\n",
    "def generator_train_step(fake_data):\n",
    "    g_optimizer.zero_grad()\n",
    "    prediction = discriminator(fake_data)\n",
    "    error = loss(prediction.squeeze(), torch.ones(len(real_data)).to(device))\n",
    "    error.backward()\n",
    "    g_optimizer.step()\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例化 并定义 优化器 和 损失函数\n",
    "\n",
    "利用交叉熵做所损失函数，学习率为0.0002的Adam函数为优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vejwiYhwgIe4"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)\n",
    "loss = nn.BCELoss()\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "- 加载真实数据并通过生成网络生成虚假的数据。\n",
    "- 利用discriminator_train_step训练判别器\n",
    "- 生成一组新的假数据，通过generator_train_step优化生成网络\n",
    "\n",
    "并且统计这些损失值，在最后进行展示，查看网络的优化的一个过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 915
    },
    "id": "k4YuDDoLgKOc",
    "outputId": "75e97f2f-d5f0-490b-a0d6-42f29749a077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "log = Report(10)\n",
    "for epoch in range(10):\n",
    "    print(epoch)\n",
    "    N = len(dataloader)\n",
    "    for i, images in enumerate(dataloader):\n",
    "        real_data = images.to(device)\n",
    "        fake_data = generator(torch.randn(len(real_data), 100, 1, 1).to(device)).to(device)\n",
    "        fake_data = fake_data.detach()\n",
    "        d_loss = discriminator_train_step(real_data, fake_data)\n",
    "        fake_data = generator(torch.randn(len(real_data), 100, 1, 1).to(device)).to(device)\n",
    "        g_loss = generator_train_step(fake_data)\n",
    "        log.record(epoch+(1+i)/N, d_loss=d_loss.item(), g_loss=g_loss.item(), end='\\r')\n",
    "    log.report_avgs(epoch+1)\n",
    "    print(epoch + ':' + d_loss + g_loss)\n",
    "log.plot_epochs(['d_loss','g_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程完成后，使用以下代码生成图像样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "blr9dMTOgSZZ",
    "outputId": "1cfe0b20-f5bf-45ef-fdd8-cd6801f23372"
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "noise = torch.randn(64, 100, 1, 1, device=device)\n",
    "sample_images = generator(noise).detach().cpu()\n",
    "grid = vutils.make_grid(sample_images, nrow=8, normalize=True)\n",
    "show(grid.cpu().detach().permute(1,2,0), sz=10, title='Generated images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxVwIwblmCXY"
   },
   "source": [
    "## 练习："
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Face_generation_using_DCGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
